---
tags: ['LessWrong', 'Concept']
src: https://www.lesswrong.com/tag/superintelligence
---

# Superintelligence
A Superintelligence is a being with superhuman intelligence, and a focus of the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri)'s research. Specifically, Nick Bostrom (1997) defined it as

The [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) is dedicated to ensuring humanity's safety and prosperity by preparing for the development of an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) with superintelligence. Given its intelligence, it is likely to be [incapable of being controlled](https://www.lesswrong.com/tag/ai-boxing-containment) by humanity. It is important to prepare early for the development of [friendly artificial intelligence](https://www.lesswrong.com/tag/friendly-artificial-intelligence), as there may be an [AI arms race](https://www.lesswrong.com/tag/ai-arms-race). A strong superintelligence is a term describing a superintelligence which is not designed with the same architecture as the human brain....[(Read More)]()

