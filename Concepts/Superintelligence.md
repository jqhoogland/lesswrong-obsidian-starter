---
tags: ['LessWrong', 'Concept']
src: https://www.lesswrong.com/tag/superintelligence
---

# Superintelligence
A Superintelligence is a being with superhuman intelligence, and a focus of the [[MIRI|Machine Intelligence Research Institute]]'s research. Specifically, Nick Bostrom (1997) defined it as

The [[MIRI|Machine Intelligence Research Institute]] is dedicated to ensuring humanity's safety and prosperity by preparing for the development of an [[Artificial General Intelligence]] with superintelligence. Given its intelligence, it is likely to be [[AI Boxing (Containment)|incapable of being controlled]] by humanity. It is important to prepare early for the development of [[Friendly Artificial Intelligence|friendly artificial intelligence]], as there may be an [[AI Arms Race|AI arms race]]. A strong superintelligence is a term describing a superintelligence which is not designed with the same architecture as the human brain....[(Read More)]()

